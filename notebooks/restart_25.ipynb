{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f68c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://router.huggingface.co/v1/chat/completions\n",
      "meta-llama/Llama-3.1-8B-Instruct:cerebras\n"
     ]
    }
   ],
   "source": [
    "# use magic command to auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))  # parent folder of src\n",
    "from src.simulation import run_simulation\n",
    "from src.save import save_replicas_raw\n",
    "import dotenv\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# load PARAMS.json\n",
    "with open(\"PARAMS.json\", \"r\") as f:\n",
    "    PARAMS = json.load(f)\n",
    "\n",
    "PARAMS[\"time_decay_rate\"] =  np.log(2)/2  # halving the weight every 2 time steps. change /2 -> /k for halving every k time steps.\n",
    "PARAMS[\"W_agent_success\"] = 1\n",
    "PARAMS[\"W_personal_weights\"] = 1\n",
    "PARAMS[\"W_post_success\"] = 1\n",
    "PARAMS[\"noise_level\"] = 0.01\n",
    "\n",
    "\n",
    "\n",
    "ENV = dotenv.dotenv_values(\"./.env\")\n",
    "HF_TOKEN = ENV[\"HF_TOKEN\"]\n",
    "API_URL = ENV[\"API_URL\"]\n",
    "MODEL = ENV[\"MODEL\"]\n",
    "\n",
    "\n",
    "print(API_URL)\n",
    "print(MODEL)\n",
    "\n",
    "# add HF_TOKEN to PARAMS\n",
    "PARAMS[\"HF_TOKEN\"] = HF_TOKEN\n",
    "PARAMS[\"API_URL\"] = API_URL\n",
    "PARAMS[\"MODEL\"] = MODEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcafd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 4\n",
    "num_agents = 36\n",
    "fill_history = 1\n",
    "\n",
    "\n",
    "PARAMS[\"num_agents\"] = num_agents\n",
    "PARAMS[\"timesteps\"] = timesteps\n",
    "PARAMS[\"fill_history\"] = fill_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e685be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_current: 1\n",
      "T_current: 2\n",
      "T_current: 3\n",
      "T_current: 4\n",
      "T_current: 1\n",
      "T_current: 2\n",
      "T_current: 3\n",
      "T_current: 4\n"
     ]
    }
   ],
   "source": [
    "List_of_WEIGHTS, List_of_READ_MATRIX, List_of_LIKES, List_of_POSTS = await run_simulation(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1dab3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_root = \"runs2\"\n",
    "save_replicas_raw(\n",
    "    out_root,\n",
    "    List_of_WEIGHTS, List_of_READ_MATRIX, List_of_LIKES, List_of_POSTS,\n",
    "    PARAMS,\n",
    "    extra_meta={\"note\": \"raw generation only, no metrics\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8683cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, csv, time, uuid, hashlib\n",
    "import numpy as np\n",
    "\n",
    "def _flatten_posts_with_replica(List_of_POSTS):\n",
    "    \"\"\"Yield (replica, agent, t, text) across all replicas.\"\"\"\n",
    "    for r, POSTS in enumerate(List_of_POSTS):\n",
    "        for agent_id, row in enumerate(POSTS):\n",
    "            for t, txt in enumerate(row):\n",
    "                if not txt:\n",
    "                    continue\n",
    "                s = str(txt).strip()\n",
    "                if s:\n",
    "                    yield (r, agent_id, t, s)\n",
    "\n",
    "def _sha1_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    import hashlib\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def save_multi_results(\n",
    "    out_dir,\n",
    "    List_of_WEIGHTS,\n",
    "    List_of_READ_MATRIX,\n",
    "    List_of_LIKES,\n",
    "    List_of_POSTS,\n",
    "    PARAMS,\n",
    "    prompts_yaml_path=\"prompts.yaml\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a multi-replica run:\n",
    "\n",
    "    - arrays_multi.npz: WEIGHTS, READ_MATRIX, LIKES stacked on axis=0 (replica)\n",
    "    - posts.csv: long format with columns (replica, agent, t, text)\n",
    "    - metadata.json: params, shapes, reproducibility info\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1) Stack arrays along replica axis ---\n",
    "    WEIGHTS = np.stack(List_of_WEIGHTS, axis=0)       # (R, N, T, max_deg)\n",
    "    READ_MATRIX = np.stack(List_of_READ_MATRIX, 0)    # (R, N, T, max_deg)\n",
    "    LIKES = np.stack(List_of_LIKES, 0)                # (R, N, T)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        os.path.join(out_dir, \"arrays_multi.npz\"),\n",
    "        WEIGHTS=WEIGHTS,\n",
    "        READ_MATRIX=READ_MATRIX,\n",
    "        LIKES=LIKES,\n",
    "    )\n",
    "\n",
    "    # --- 2) Posts in tidy CSV with replica column ---\n",
    "    posts_csv = os.path.join(out_dir, \"posts.csv\")\n",
    "    with open(posts_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"replica\", \"agent\", \"t\", \"text\"])\n",
    "        w.writerows(_flatten_posts_with_replica(List_of_POSTS))\n",
    "\n",
    "    # --- 3) Metadata ---\n",
    "    meta = {\n",
    "        \"run_id\": str(uuid.uuid4()),\n",
    "        \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"params\": PARAMS,\n",
    "        \"replicas\": len(List_of_POSTS),\n",
    "        \"shapes\": {\n",
    "            \"WEIGHTS\": list(WEIGHTS.shape),\n",
    "            \"READ_MATRIX\": list(READ_MATRIX.shape),\n",
    "            \"LIKES\": list(LIKES.shape),\n",
    "        },\n",
    "        \"temperature\": PARAMS.get(\"temperature\", 0.0),\n",
    "        \"model\": PARAMS.get(\"MODEL\"),\n",
    "        \"api_url\": PARAMS.get(\"API_URL\"),\n",
    "        \"prompts_yaml\": {\n",
    "            \"path\": prompts_yaml_path,\n",
    "            \"sha1\": _sha1_file(prompts_yaml_path),\n",
    "        },\n",
    "        \"files\": {\n",
    "            \"arrays\": \"arrays_multi.npz\",\n",
    "            \"posts\": \"posts.csv\",\n",
    "        },\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_multi_results(out_dir):\n",
    "    \"\"\"Reads back what save_multi_results wrote.\"\"\"\n",
    "    arrays = np.load(os.path.join(out_dir, \"arrays_multi.npz\"), allow_pickle=True)\n",
    "    with open(os.path.join(out_dir, \"metadata.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    posts = []\n",
    "    posts_path = os.path.join(out_dir, \"posts.csv\")\n",
    "    if os.path.exists(posts_path):\n",
    "        with open(posts_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                posts.append((int(row[\"replica\"]), int(row[\"agent\"]), int(row[\"t\"]), row[\"text\"]))\n",
    "\n",
    "    return {\n",
    "        \"WEIGHTS\": arrays[\"WEIGHTS\"],\n",
    "        \"READ_MATRIX\": arrays[\"READ_MATRIX\"],\n",
    "        \"LIKES\": arrays[\"LIKES\"],\n",
    "        \"POSTS_long\": posts,\n",
    "        \"metadata\": meta,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac3ce1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replicas: 2\n",
      "WEIGHTS[0].shape: (6, 3, 5)\n",
      "READ_MATRIX[0].shape: (6, 3, 5)\n",
      "LIKES[0].shape: (6, 3)\n",
      "example post r0 a0 t0: \"Just learned that our local school district is cutting funding for arts and music programs. This is a huge step back for our kids and I'm calling on our school board to reverse this decision!\" #SaveTheArts #PrioritizeEducation\n"
     ]
    }
   ],
   "source": [
    "R = len(List_of_POSTS)\n",
    "print(\"replicas:\", R)\n",
    "print(\"WEIGHTS[0].shape:\", List_of_WEIGHTS[0].shape)   # (N, T_total, max_deg)\n",
    "print(\"READ_MATRIX[0].shape:\", List_of_READ_MATRIX[0].shape)  # (N, T_total, max_deg)\n",
    "print(\"LIKES[0].shape:\", List_of_LIKES[0].shape)       # (N, T_total)\n",
    "print(\"example post r0 a0 t0:\", List_of_POSTS[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee260b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "WEIGHTS = np.stack(List_of_WEIGHTS, axis=0)      # (R, N, T, max_deg)\n",
    "READM   = np.stack(List_of_READ_MATRIX, axis=0)  # (R, N, T, max_deg)\n",
    "LIKES   = np.stack(List_of_LIKES, axis=0)        # (R, N, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeaf041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92509651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd5b2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 replicas available\n",
      "Array shapes:\n",
      "WEIGHTS: (6, 3, 5)\n",
      "READ_MATRIX: (6, 3, 5)\n",
      "LIKES: (6, 3)\n",
      "\n",
      "First 5 posts:\n",
      "(0, 0, '\"Just learned that our local school district is cutting funding for arts and music programs. This is a huge step back for our kids and I\\'m calling on our school board to reverse this decision!\" #SaveTheArts #PrioritizeEducation')\n",
      "(0, 1, '\"Just had a chat with a fellow parent who\\'s concerned about the school board\\'s priorities. We agree that we need to stand up for our kids and their education, but we also need to cut wasteful spending! It\\'s time to get our priorities straight and make a change in our community\" #SensibleSolutions #TaxpayerFirst')\n",
      "(0, 2, '\"It\\'s time to hold our school board accountable for their decisions. We need to make sure they\\'re putting our kids\\' needs first, not advancing a radical agenda. Let\\'s demand transparency and action at the next school board meeting!\" #ParentsUnite #AccountabilityMatters')\n",
      "(1, 0, '\"Stood in line for 5 hours to cast my vote in the last election. Protected our borders, stood up for traditional values. #MAGA\"')\n",
      "(1, 1, '\"Just saw a post saying we should let in more refugees. Can\\'t believe the ignorance of some people. Our country\\'s safety is more important than their feelings #BuildTheWall\"')\n"
     ]
    }
   ],
   "source": [
    "manifest = load_manifest(\"runs/2025-09-08_exp1\")\n",
    "print(manifest[\"replicas\"], \"replicas available\")\n",
    "\n",
    "# load first replica\n",
    "r0 = load_replica(\"runs/2025-09-08_exp1/r000\")\n",
    "\n",
    "print(\"Array shapes:\")\n",
    "print(\"WEIGHTS:\", r0[\"WEIGHTS\"].shape)\n",
    "print(\"READ_MATRIX:\", r0[\"READ_MATRIX\"].shape)\n",
    "print(\"LIKES:\", r0[\"LIKES\"].shape)\n",
    "\n",
    "print(\"\\nFirst 5 posts:\")\n",
    "for row in r0[\"POSTS_long\"][:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94b0251",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m pst = List_of_POSTS[\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m msg0 = pst[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "pst = List_of_POSTS[0]\n",
    "msg0 = pst[0]\n",
    "for i in len(msg0):\n",
    "    print(msg0[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51315d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monks-vs-dragons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
